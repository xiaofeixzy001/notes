[TOC]

# 概述

生产环境中会产生大量的系统日志、应用程序日志、安全日志等，通过对日志的分析可以了解服务器的负载、健康状况，可以分析客户的分布情况、客户的行为，甚至基于这些分析可以做出预测。

# 采集流程

日志产出 - 采集 - 存储 - 分析 - 存储 - 可视化

采集工具：Logstash, Flume, Scribe

存储工具：数据库, NoSQL

# ELK

开源的实施日志分析平台

Logstash收集日志并存放到ElasticSearch,Kibana则从ES集群中查询数据生成图表,返回web端

# 分析前提

## 结构化数据

mysql等

## 半结构化数据

日志文件, html日志文件，xml日志文件等

非结构化数据

音视频文件等

## 文本分析

日志是文本文件，需要依赖文件IO、字符操作、正则等技术提取所需的日志内容

# 提取数据

## 空格分割

简单的提取：

按空格为分隔符提取每部分数据

 

```
# 日志格式
s = """
123.125.71.36 - - [06/Apr/2017:18:09:25 +0800] "GET / HTTP/1.1" 200 8642 "-" "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"
"""

for line in s.split():
    print(line)

"""
123.125.71.36
-
-
[06/Apr/2017:18:09:25
+0800]
"GET
/
HTTP/1.1"
200
8642
"-"
"Mozilla/5.0
(compatible;
Baiduspider/2.0;
+http://www.baidu.com/search/spider.html)"
"""
```

缺点：

数据并没有按照业务分割号，比如时间和时区就被分开了，URL相关的也被分开了，User Agent的空格最多，被分割了，处理后的结果不符合需求。

解决办法：

可以修改日志生成的格式，修改默认的空格分隔符，改成特定的分隔符就会方便很多，比如'\x01'这个不可见的ASCII；

## 空格分割改进

假如不修改默认分隔符，而又想要按照需要来提取，比如按照空格分割，但遇到引号或中括号特殊处理一下就行了。

思路：

先按照空格切分，然后一个个字符迭代，但如果发现是[或"，就不在判断是否空格,知道]或右引号出现，中间的数据就算出现空格也不再处理。

 

```
s = """
123.125.71.36 - - [06/Apr/2017:18:09:25 +0800] "GET / HTTP/1.1" 200 8642 "-" "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"
"""

ch = set(""" -\t""")

def makekey(key: str, chars=ch):
    start = 0  # 控制索引位置
    skip = False  # 用来控制进入特殊处理
    for i, c in enumerate(key):
        if not skip and c in '"[':  # 遇到第一个[或“
            start = i + 1  # 遇到则跳过这个位置
            skip = True
        elif skip and c in ']"':  # 第二个]或"
            skip = False
            yield key[start:i]  # 返回[]之间的数据
            start = i + 1
            continue
        if skip:
            continue

        if c in chars:
            if start == i:  # 如果2个特殊字符相临,则start一定等于i
                start = i + 1
                continue
            yield key[start:i]
            start = i + 1  # 因为i是特殊字符c的索引,不需要,所以跳过
    else:
        if start < len(key):  # 小于说明还有有效的字符,且一直到末尾
            yield key[start:]

# 测试
for x in makekey(s):
    print(x)

# 文件测试
with open('logs/test.log') as f:
    res = f.readline()
    for x in makekey(res):
        print(x)
"""
123.125.71.36
06/Apr/2017:18:09:25 +0800
GET / HTTP/1.1
200
8642
-
Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)
"""
```

## 数据处理

对提取出来的数据进行处理

比如field中的数据都是由类型的，例如时间、状态码等，对不同的field要做不同的类型转换，甚至是自定义的转换。

例如时间转换，使用datetime类的strptime方法

状态码是整型，使用int转换

请求信息的解析，转换成字典

最后对每一个字段命名，然后与值和类型转换的方法对应，注意解析顺序。

最终输出格式：

{'remote': '123.125.71.36', 

'-': '-', 

'datetime': datetime.datetime(2017, 4, 6, 18, 9, 25, tzinfo=datetime.timezone(datetime.timedelta(0, 28800))), 

'protocol': {'method': 'GET', 'url': '/', 

'protocol': 'HTTP/1.1'}, 

'status': 200, 'size': 8642, 

'useragent': 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)'}

 

```
import datetime

s = '''123.125.71.36 - - [06/Apr/2017:18:09:25 +0800] "GET / HTTP/1.1" 200 8642 "-" "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"'''
ch = set(""" \t""")  # 需要过滤的符号
def makekey(key: str, chars=ch):
    start = 0
    skip = False
    for i, c in enumerate(key):
        if not skip and c in '"[':  # 遇到第一个[或“
            start = i + 1
            skip = True
        elif skip and c in ']"':  # 第二个]或"
            skip = False
            yield key[start:i]
            start = i + 1
            continue
        if skip:
            continue

        if c in chars:
            if start == i:  # 如果2个特殊字符相临,则start一定等于i
                start = i + 1
                continue
            yield key[start:i]
            start = i + 1  # 因为i是特殊字符c的索引,不需要,所以跳过
    else:
        if start < len(key):  # 小于说明还有有效的字符,且一直到末尾
            yield key[start:]

# 时间转换
def convert_time(timestr):
    """
    将时间字符串转换成datetime对象: 2017-04-06 18:09:25+08:00
    可简写为: lambda timestr: datetime.datetime.strptime(timestr, '%d/%b/%Y:%H:%M:%S %z')
    :param timestr:
    :return:
    """
    return datetime.datetime.strptime(timestr, '%d/%b/%Y:%H:%M:%S %z')

# 请求信息转换
def get_req(req: str):
    """
    将2个列表拉链合并,并转换为dict: {'method': 'GET', 'url': '/', 'protocol': 'HTTP/1.1'}
    可简写为: lambda req: dict(zip(['method', 'url', 'protocol'], req.split()))
    :param req:
    :return: lambda函数对象地址
    """
    return dict(zip(['method', 'url', 'protocol'], req.split()))

# 定义key
names = ('remote', '-', '-', 'datetime', 'protocol', 'status', 'size', '-', 'useragent',)

# 定义value
ops = (None, None, None,
       lambda timestr: datetime.datetime.strptime(timestr, '%d/%b/%Y:%H:%M:%S %z'),
       lambda req: dict(zip(('method', 'url', 'protocol'), req.split())),
       int, int, None, None,)

# 将key和value生成字典
# dic = {}
# for name, op, data in zip(names, ops, makekey(s)):
#     # dic[name] = data
#     if op is None:
#         dic[name] = data
#     else:
#         dic[name] = op(data)
# print(dic)

def extract(key: str):
    return {name: data if op is None else op(data) for name, op, data in zip(names, ops, makekey(key))}

print(extract(s))
```

## 正则提取

 

```
import re
import datetime

s = '''123.125.71.36 - - \
[06/Apr/2017:18:09:25 +0800] \
"GET / HTTP/1.1" \
200 \
8642 \
"-" \
"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"'''

pattern = '''(?P<remote>[\d\.]{7,})\
\s-\s-\s\[(?P<datetime>[^\[\]]+)\]\s\
"(?P<protocol>[^"]+)"\
\s(?P<status>\d{3})\s(?P<size>\d+)\s\
"[^"]+"\s"(?P<useragent>[^"]+)"'''

# 将protocol拆分出method,url,protocol
pattern1 = '''(?P<remote>[\d\.]{7,})\
\s-\s-\s\[(?P<datetime>[^\[\]]+)\]\
\s"(?P<method>\w+)\
\s(?P<url>\/[\w\/\?=\.]*)\
\s(?P<protocol>\w+\/\d\.\d)\
"\s(?P<status>\d+)\
\s(?P<size>\d+)\
\s"[^"]+"\s"(?P<useragent>[^"]+)"'''

ops = {
    'datetime': lambda timestr: datetime.datetime.strptime(timestr, '%d/%b/%Y:%H:%M:%S %z'),
    'status': int,
    'size': int
}

regex = re.compile(pattern)

def extract(line: str) -> dict:
    matcher = regex.match(line)
    return {k: ops.get(k, lambda x: x)(v) for k, v in matcher.groupdict().items()}

print(extract(s))
```

正则表达式：

(?<remote>[\d\.]{7,})\s-\s-\s\[(?<datetime>[^\[\]]+)\]\s"(?<method>\w+)\s(?<url>\/[\w\/\?=\.]*)\s(?<protocol>\w+\/\d\.\d)"\s(?<status>\d+)\s(?<size>\d+)\s"[^"]+"\s"(?<useragent>[^"]+)"

# 总结

目前仅仅实现对日志格式的字符串进行过滤处理，如果是日志文件呢？如果日志文件内的数据数量很多，想要看指定一段时间内的数据又该如何改进呢？