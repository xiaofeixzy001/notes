[TOC]













## 工作流程

Scrapy 是基于twisted框架开发而来，twisted是一个流行的事件驱动的python网络框架。因此Scrapy使用了一种非阻塞（又名异步）的代码来实现并发。



![img](Scrapy%E6%A1%86%E6%9E%B6.assets/1376305-20190310112649470-805577214.png)

1、引擎(EGINE)
引擎负责控制系统所有组件之间的数据流，并在某些动作发生时触发事件。有关详细信息，请参见上面的数据流部分。

2、调度器(SCHEDULER)
用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL的优先级队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址

3、下载器(DOWLOADER)
用于下载网页内容, 并将网页内容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的

4、爬虫(SPIDERS)
SPIDERS是开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求

5、项目管道(ITEM PIPLINES)
在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作
下载器中间件(Downloader Middlewares)位于Scrapy引擎和下载器之间，主要用来处理从EGINE传到DOWLOADER的请求request，已经从DOWNLOADER传到EGINE的响应response，

6、爬虫中间件(Spider Middlewares)
位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入（即responses）和输出（即requests）





## 项目操作

scrapy操作的基本流程如下：

- 创建项目：scrapy startproject 项目名称
- 新建爬虫：scrapy genspider 爬虫文件名 爬虫基础域名
- 编写item
- spider最后return item
- 在setting中修改pipeline配置
- 在对应pipeline中进行数据持久化操作

## 创建

命令行下执行

```shell
scrapy startproject books
scrapy genspider books
```







# scrapy框架+selenium的使用

## 使用情景: 　　

　　在通过scrapy框架进行某些网站数据爬取的时候，往往会碰到页面动态数据加载的情况发生，如果直接使用scrapy对其url发请求，是绝对获取不到那部分动态加载出来的数据值。但是通过观察我们会发现，通过浏览器进行url请求发送则会加载出对应的动态加载出的数据。那么如果我们想要在scrapy也获取动态加载出的数据，则必须使用selenium创建浏览器对象，然后通过该浏览器对象进行请求发送，获取动态加载的数据值

## 使用流程

重写爬虫文件的__init__()构造方法，在该方法中使用selenium实例化一个浏览器对象（因为浏览器对象只需要被实例化一次）;

重写爬虫文件的closed(self,spider)方法，在其内部关闭浏览器对象,该方法是在爬虫结束时被调用;

重写下载中间件的process_response方法，让该方法对响应对象进行拦截，并篡改response中存储的页面数据;

在settings配置文件中开启下载中间件;











