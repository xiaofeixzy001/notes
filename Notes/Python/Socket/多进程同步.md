[TOC]

# **进程同步(Lock)**

加锁的目的是为了保证多个进程修改同一块数据时,同一时间只能有一个修改,即串行的修改,这样做速度是慢了,但是保证了数据的安全和准确.

## 实例

抢票系统,10个人抢一张票.

 

```
# 文件a.txt内容:{"count": 1}
# json仅能识别双引号

from multiprocessing import Process, Lock
import json
import time
import random
import os

def work(filename):
    with open(filename, encoding='utf-8') as f:
        dic = json.loads(f.read())
        print('剩余票数: %s' % dic['count'])
    if dic['count'] > 0:
        dic['count'] -= 1
        time.sleep(random.randint(0,2))  # 模拟网络延迟
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(json.dumps(dic))
        print('\033[43m%s 购票成功.\033[0m' % os.getpid())
    else:
        print('\033[45m%s 购票失败.\033[0m' % os.getpid())

if __name__ == '__main__':
    p_l = []
    for i in range(10):
        p = Process(target=work, args=('a.txt',))
        p_l.append(p)
        p.start()
    for p in p_l:
        p.join()

    print('主线程.')
```

运行结果是有好几个人抢到了票,这是因为多个进程共同读写同一个文件导致,解决办法就是加锁,修改如下:

 

```
def work(filename, lock):
    with lock: # 上下文管理
      # lock.acquire()  # 加锁
        with open(filename, encoding='utf-8') as f:
            dic = json.loads(f.read())
            # print('剩余票数: %s' % dic['count'])
        if dic['count'] > 0:
            dic['count'] -= 1
            time.sleep(random.randint(0,2))  # 模拟网络延迟
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(json.dumps(dic))
            print('\033[43m%s 购票成功.\033[0m' % os.getpid())
        else:
            print('\033[45m%s 购票失败.\033[0m' % os.getpid())
      # lock.release()  # 释放锁
```

这样就可以保证只有一个人可以抢到票,但是速度会降低.

# **进程池Pool()**

开多进程的目的是为了并发,如果有多核,通常有几个核就开几个进程,进程开启过多,效率反而会下降(开进程会占用系统资源),但很明显需要并发执行的任务要远大于cpu核数,这时我们就可以通过维护一个进程池来控制进程数目,比如httpd的进程模式,规定好最小进程数和最大进程数.

Pool可以提供指定数量的进程,当有新的请求提交到pool中时,如果池子还没满,则创建新的进程来执行请求,如果满了,那么就会让请求处于阻塞等待状态,直到池中有进程结束,比如上厕所,一共5个坑,坑满了,别人只能在外面等.

创建进程池的类:

multiprocessing.Pool(number, func, args):创建进程池

## 参数

number:要创建的进程数，如果省略，将默认使用cpu_count()的值

func：是每个工作进程启动时要执行的工作,可调用对象，默认为None

args：传参

## 方法

p=multiprocessing.Pool()

p.apply(self, func, args=(), kwds={}):阻塞执行，一个个执行。

在一个池工作进程中执行func(*args,**kwargs),然后返回结果。

同步.需要强调的是：此操作并不会在所有池工作进程中并执行func函数。如果要通过不同参数并发地执行func函数，必须从不同线程调用p.apply()函数或者使用p.apply_async()。

p.apply_async(self, func, args=(), kwds={}, callback=None, error_callback=None):

在一个池工作进程中执行func(*args,**kwargs),然后返回结果。异步.此方法的结果是AsyncResult类的实例，callback是可调用对象，接收输入参数。当func的结果变为可用时，将理解传递给callback。callback禁止执行任何阻塞操作，否则将接收其他异步操作中的结果。

p.close():

关闭进程池，不能在接受新的任务。如果所有操作持续挂起，它们将在工作进程终止前完成。

P.join():

等待所有工作进程退出。此方法只能在close()或teminate()之后调用。

方法apply_async()和map_async()的返回值是AsyncResul的实例obj。

实例具有以下方法：

obj.get():返回结果，如果有必要则等待结果到达。timeout是可选的。如果在指定时间内还没有到达，将引发一场。如果远程操作中引发了异常，它将在调用此方法时再次被引发。

obj.ready():如果调用完成，返回True

obj.successful():如果调用完成且没有引发异常，返回True，如果在结果就绪之前调用此方法，引发异常

obj.wait([timeout]):等待结果变为可用。

obj.terminate()：立即终止所有工作进程，不再处理未处理的任务。同时不执行任何清理或结束任何挂起工作。如果p被垃圾回收，将自动调用此函数

## 应用实例

1 同步和异步

 

```
# 定义进程任务
def work(n):
    print('开工了..')
    time.sleep(2)
    return n ** 2

if __name__ == '__main__':
    # 方式一:同步apply用法:主进程会一直等apply提交的任务结束后才继续执行后面的代码
    # 默认进程池数量为cpu核数.查看cpu核心数:print(os.cpu_count())
    q = Pool()

    # 开一个进程,添加到进程池中,并等待任务执行完work任务后,继续往下执行,同步有返回值
    res = q.apply(work, args=(2,))
    print(res)

    # 方式二:异步apply_async用法:主进程需要使用join,等待进程池内的任务都处理完,然后可以使用get()开得到任务的结果,否则,主进程结束,进程池可能还没来得及执行进程池中的任务,也跟着一起结束掉了.
    q = Pool()
    res = q.apply_async(work, args=(4,))
    q.close()
    q.join()  # 等待进程池中的进程全部执行完
    print(res.get())  # 获取res进程的任务执行的结果
```

2 多个进程池示例

 

```
import multiprocessing
import os, time, random

def Lee():
    print('\nRun task Lee-%s' % (os.getpid()))
    start = time.time()
    time.sleep(random.random() * 10) # random.random()随机生成0-1之间的小数
    end = time.time()
    print('Task Lee, runs %0.2f seconds.' % (end - start)) # %0.2f表示格式化为浮点数,保留2位小数

def Marlon():
    print('\nRun task Marlon-%s' % (os.getpid()))
    start = time.time()
    time.sleep(random.random() * 40)
    end = time.time()
    print('Task Marlon runs %0.2f seconds.' % (end - start))

if __name__ == '__main__':
    function_list = [Lee, Marlon]
    print('Parent process: %s' % (os.getpid()))

    pool = multiprocessing.Pool()
    for func in function_list:
        pool.apply_async(func)

    print('Waiting for all subprocesses done..')
    pool.close()
    pool.join()
    print('All subprocesses was done..')
```

注意: 调用join之前,一定要先调用close()函数,否则会出错,close()执行后不会有新的进程加入到pool,join函数等待所有子进程结束。

3 使用进程池维护固定数目的进程

验证:

Pool()不指定最大进程数,默认为cpu核数(os.cpu_count())

开启6个客户端连接服务端,会发现后2个客户端处于阻塞状态

在每个进程内查看pid,会发现pid总是为4个,多个客户端共用4个进程

 

```
# 服务端:
from socket import *
from multiprocessing import Pool
import os

server=socket(AF_INET,SOCK_STREAM)
server.setsockopt(SOL_SOCKET,SO_REUSEADDR,1)
server.bind(('127.0.0.1',8080))
server.listen(5)

def talk(conn,client_addr):
    print('进程pid: %s' %os.getpid())
    while True:
        try:
            msg=conn.recv(1024)
            if not msg:break
            conn.send(msg.upper())
        except Exception:
            break

if __name__ == '__main__':
    p=Pool(1)
    while True:
        conn,client_addr=server.accept()
        p.apply_async(talk,args=(conn,client_addr))
      # p.apply(talk,args=(conn,client_addr)) #同步的话，则同一时间只有一个客户端能访问

# 客户端1:
from socket import *
client = socket(AF_INET,SOCK_STREAM)
client.connect(('127.0.0.1', 8080))

while True:
    msg = input('>> ').strip()
    if not msg: continue
    client.send(msg.encode('utf-8'))

    msg = client.recv(1024)
    print(msg.decode('utf-8'))

# 客户端2:
from socket import *
client = socket(AF_INET,SOCK_STREAM)
client.connect(('127.0.0.1', 8080))

while True:
    msg = input('>> ').strip()
    if not msg: continue
    client.send(msg.encode('utf-8'))

    msg = client.recv(1024)
    print(msg.decode('utf-8'))

# 客户端3:
# 客户端4:
# 客户端5:
# 客户端6:
```

## 多进程、多线程选择

1 CPU密集型

CPython中使用到了GIL，多线程的时候锁互相竞争，且多核优势不能发挥，Python多进程效率更高。

2 IO密集型

适合用多线程，可以减少多进程间IO的序列化开销。且在IO等待的时候，切换到其他线程继续执行，效率不错。

## 应用

请求/应答模型：WEB应用中常见的处理模型

master启动多个worker工作进程，一般和CPU数目相同，发挥多核优势。

worker工作进程中，往往需要操作网络IO和磁盘IO，启动多线程，提高并发处理能力。worker处理用户的请求，往往需要等待数据，处理完请求还要通过网络IO返回响应。

这就是Nginx工作模式。

**回调函数**

apply_async的扩展用法

回调函数:

执行一个任务的结果,交给另外一个任务继续执行.

1,无需回调函数场景:如果在主进程中,等待进程池中所有任务都执行完毕后,再统一处理结果,则无需回调函数;

2,需要回调函数场景:进程池中任何一个任务,一旦处理完了,就立即告诉主进程:我处理完了,你可以取我的结果了,主进程则会调用一个函数去处理该结果,该函数即为回调函数.

我们可以把耗时间（阻塞）的任务放到进程池中，然后指定回调函数（主进程负责执行），这样主进程在执行回调函数时就省去了I/O的过程，直接拿到的是任务的结果

## 实例:爬网页

 

```
from multiprocessing import Process, Pool
import time, random
import requests  # requests 需要额外安装
import re, json

def get_page(url, pattern):
    """
    爬取网页,并返回网页的内容
    传给回调函数parse_page
    :param url: 
    :param pattern: 
    :return: 
    """
    response = requests.get(url)
    if response.status_code == 200:  # 判断是否爬取成功
        return (response.text, pattern)  # response.text获取url的内容

def parse_page(info):
    page_content, pattern = info
    res = re.findall(pattern, page_content)  # 按照正则表示式将内容匹配出来
    
    # 将匹配后的内容,重新自定一个字典,按照自己的格式存储数据:
    for item in res:
        dic = {
            'index': item[0],
            'title': item[1],
            'actor': item[2].strip()[3:],
            'time': item[3][5:],
            'score': item[4] + item[5]
        }
        # 将最后爬取并处理的结果存储到文件中
        with open('db.txt', 'a', encoding='utf-8') as f:
            f.write('%s\n' % json.dumps(dic))

if __name__ == '__main__':
    # 针对不同网页,应用不同的正则表达式
    # 正则表达式不太懂,需要复习.
    pattern1 = re.compile(r'<dd>.*?board-index.*?>(\d+)<.*?title="(.*?)".*?star.*?>(.*?)<.*?releasetime.*?>(.*?)<.*?integer.*?>(.*?)<.*?fraction.*?>(.*?)<',re.S)
    #pattern2 = 
    #pattern3 = 
    #pattern4 = 
    
    url_dic = {
        'http://maoyan.com/board/7': pattern1,
    }

    p = Pool() # 开一个进程池,4个
    res_l = []
    for url, pattern in url_dic.items():
        # 异步处理,进程先执行get_page任务,将结果传给回调函数parse_page处理,获取到的最终结果赋值给res
        res = p.apply_async(get_page, args=(url, pattern), callback=parse_page)
        res_l.append(res)

    for i in res_l:
        i.get()  # 获取res_l的内容
```

爬取结果存到db.txt中,内容如下:

{"time": "2017-06-16", "index": "1", "actor": "\u674e\u5fae\u6f2a,\u4ea6\u98ce", "title": "\u91cd\u8fd4\u00b7\u72fc\u7fa4", "score": "9.3"}

{"time": "2017-06-30", "index": "2", "actor": "\u738b\u9719,\u4fdd\u5251\u950b,\u8463\u52c7", "title": "\u8840\u6218\u6e58\u6c5f", "score": "9.2"}

